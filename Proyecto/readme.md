# üìò Sistema Distribuido de Pr√©stamo, Renovaci√≥n y Devoluci√≥n de Libros

Autores: Gabriel Jaramillo Cuberos, Roberth M√©ndez Rivera, Mariana Osorio V√°squez, Juan Esteban Vera Garz√≥n 

## üß© Descripci√≥n general
Este proyecto implementa un sistema distribuido para la gesti√≥n de pr√©stamos, devoluciones y renovaciones de libros en una biblioteca con m√∫ltiples sedes.
La arquitectura se basa en ZeroMQ (JeroMQ para Java) y usa los patrones REQ/REP y PUB/SUB para permitir comunicaci√≥n entre los componentes.

## üèóÔ∏è Arquitectura del sistema
El diagrama arquitect√≥nico muestra la estructura global del sistema distribuido de pr√©stamo de libros y la relaci√≥n entre sus principales componentes desplegados en dos sedes. 

Cada sede cuenta con: 
- Un Gestor de Carga (GC) que recibe las solicitudes de los clientes y las publica hacia los actores.
- Dos Actores especializados: uno para renovaciones y otro para devoluciones, que consumen los mensajes del GC mediante el patr√≥n PUB/SUB.
- Un Gestor de Almacenamiento (GA) responsable de mantener la base de datos local y sincronizar los cambios con su r√©plica en la otra sede. 

Los Procesos Solicitantes (PS), ubicados en la capa de clientes, pueden conectarse a cualquiera de los GC disponibles para enviar solicitudes de renovaci√≥n o devoluci√≥n. 
La comunicaci√≥n entre los GA de ambas sedes se realiza de forma as√≠ncrona mediante replicaci√≥n, garantizando consistencia eventual. Este dise√±o distribuye la carga de procesamiento y asegura tolerancia a fallos mediante redundancia de sedes. 

```mermaid
graph LR 

subgraph Sede_1 

GC1[Gestor de Carga 1] 

A1D[Actor Devoluci√≥n 1] 

A1R[Actor Renovaci√≥n 1] 

GA1[Gestor de Almacenamiento 1<br/>BD Primaria R√©plica l√≠der] 

end 

 

subgraph Sede_2 

GC2[Gestor de Carga 2] 

A2D[Actor Devoluci√≥n 2] 

A2R[Actor Renovaci√≥n 2] 

GA2[Gestor de Almacenamiento 2<br/>BD Secundaria R√©plica seguidora] 

end 

 

subgraph Clientes 

PSs[Procesos Solicitantes N por sede] 

end 

 

%% Enlaces 

PSs -- Req Devoluci√≥n/Renovaci√≥n REQ --> GC1 

PSs -- Req Devoluci√≥n/Renovaci√≥n REQ --> GC2 

 

GC1 -- PUB topic: Devolucion --> A1D 

GC1 -- PUB topic: Renovacion --> A1R 

GC2 -- PUB topic: Devolucion --> A2D 

GC2 -- PUB topic: Renovacion --> A2R 

 

A1D -- Actualizaci√≥n --> GA1 

A1R -- Actualizaci√≥n --> GA1 

A2D -- Actualizaci√≥n --> GA2 

A2R -- Actualizaci√≥n --> GA2 

 

GA1 <-. Replicaci√≥n async .-> GA2

```
## Modelo de interacci√≥n
Los diagramas de interacci√≥n describen el flujo din√°mico de mensajes entre los procesos distribuidos para las operaciones principales: devoluci√≥n y renovaci√≥n. 

En ambos casos, la secuencia sigue el patr√≥n as√≠ncrono de confirmaci√≥n inmediata al cliente y procesamiento en segundo plano: 

1. El Proceso Solicitante (PS) env√≠a la solicitud al Gestor de Carga (GC).
2. El GC responde con un estado 202 Accepted para liberar al PS r√°pidamente y luego publica el evento en el canal ZeroMQ correspondiente (topic: renovaci√≥n o devoluci√≥n).
3. El Actor suscrito al t√≥pico recibe el mensaje, ejecuta la l√≥gica de negocio (verifica disponibilidad o n√∫mero de renovaciones) y actualiza el estado del libro en el Gestor de Almacenamiento (GA).
4. El GA confirma la operaci√≥n (OK o error) y registra el cambio en el archivo de persistencia local. 

Esta arquitectura basada en mensajer√≠a desacoplada permite alta concurrencia, resiliencia ante fallos y tiempos de respuesta bajos para el cliente. 
### Devoluci√≥n 
``` mermaid
sequenceDiagram 

participant PS 

participant GC 

participant Broker as ZeroMQ PUB/SUB 

participant ActorD as Actor Devoluci√≥n 

participant GA as Gestor Almacenamiento 

 

PS->>GC: POST /devolucion {libroId, sede, fecha} 

GC-->>PS: 202 OK (aceptada) 

GC->>Broker: PUB "devolucion" {libroId, sede, fecha} 

Broker-->>ActorD: entrega msg "devolucion" 

ActorD->>GA: updateLibroDevolucion(libroId, fecha) 

GA-->>ActorD: OK 
```
### Renovaci√≥n
```mermaid
sequenceDiagram 

participant PS 

participant GC 

participant Broker as ZeroMQ PUB/SUB 

participant ActorR as Actor Renovaci√≥n 

participant GA as Gestor Almacenamiento 

 

PS->>GC: POST /renovacion {libroId, sede, fechaActual} 

GC-->>PS: 202 OK nuevaFecha = +7d* 

GC->>Broker: PUB "renovacion" {libroId, fechaActual, nuevaFecha} 

Broker-->>ActorR: entrega msg "renovacion" 

ActorR->>GA: updateLibroRenovacion libroId, nuevaFecha m√°x. 2 renov. 

GA-->>ActorR: OK/ERROR l√≠mite 
```

## Modelo de fallos 
Este diagrama de fallos muestra los mecanismos de tolerancia implementados: 
- Los Gestores de Carga (GC1 y GC2) intercambian heartbeats peri√≥dicos para detectar ca√≠das de nodo.
- Los Gestores de Almacenamiento (GA1 y GA2) sincronizan su estado por replicaci√≥n peri√≥dica as√≠ncrona.
- Si uno de los GA falla, el otro mantiene los datos hasta restablecer la conexi√≥n.
- Cada GC registra sus eventos de error en un m√≥dulo de logs y alertas locales, que luego puede revisarse para diagn√≥stico. 

```mermaid
graph TD
  subgraph Sede_1
    GC1[GestorCarga 1]
    GA1[GestorAlmacenamiento 1]
  end

  subgraph Sede_2
    GC2[GestorCarga 2]
    GA2[GestorAlmacenamiento 2]
  end

  GC1 -- Heartbeat --> GC2
  GC2 -- Heartbeat --> GC1

  GA1 -- Replicaci√≥n peri√≥dica --> GA2
  GA2 -- Replicaci√≥n peri√≥dica --> GA1

  GC1 --> AL1[Registro de alertas y logs]
  GC2 --> AL2[Registro de alertas y logs]
```

## Modelo de seguridad 
``` mermaid
graph LR
  PS[Proceso Solicitante PS]
  GC[Gestor de Carga GC]
  A[Actores Renovaci√≥n / Devolucion]
  GA[Gestor de Almacenamiento GA]

  PS -- Comunicaci√≥n segura TLS/SSL --> GC
  GC -- Canal cifrado PUB/SUB --> A
  A -- Autenticaci√≥n y validaci√≥n --> GA
  GA -- Logs cifrados --> PS
```

## Diagrama de componentes 
Representa los m√≥dulos f√≠sicos de software desplegados en cada m√°quina. Cada sede replica la misma estructura l√≥gica (GC + Actores + GA). 

La sincronizaci√≥n entre GA1 y GA2 se realiza de manera as√≠ncrona, garantizando consistencia eventual. 
``` mermaid
graph LR
  subgraph Cliente
    PS[Proceso Solicitante]
  end

  subgraph Sede_1
    GC1[Gestor de Carga]
    A1R[Actor Renovaci√≥n]
    A1D[Actor Devoluci√≥n]
    GA1[Gestor de Almacenamiento]
  end

  subgraph Sede_2
    GC2[Gestor de Carga]
    A2R[Actor Renovaci√≥n]
    A2D[Actor Devoluci√≥n]
    GA2[Gestor de Almacenamiento]
  end

  PS --> GC1
  PS --> GC2
  GC1 --> A1R
  GC1 --> A1D
  A1R --> GA1
  A1D --> GA1
  GC2 --> A2R
  GC2 --> A2D
  A2R --> GA2
  A2D --> GA2
  GA1 <-. Sincronizaci√≥n .-> GA2
```

## üñ•Ô∏è Despliegue
### Diagrama de despliegue
El diagrama de despliegue representa la distribuci√≥n f√≠sica de los componentes del sistema sobre diferentes m√°quinas de la red. 
- M√°quina A (Sede 1): ejecuta el GC1, el Actor de Renovaci√≥n 1, el Actor de Devoluci√≥n 1 y el GA1 (que contiene la base de datos primaria o r√©plica l√≠der).
- M√°quina B (Sede 2): ejecuta el GC2, el Actor de Renovaci√≥n 2, el Actor de Devoluci√≥n 2 y el GA2 (r√©plica seguidora).
- M√°quina C (Clientes): aloja varios Procesos Solicitantes (PS) que generan carga de solicitudes hacia las sedes.   

La comunicaci√≥n entre PS y GC utiliza el patr√≥n REQ/REP, mientras que la comunicaci√≥n entre GC y Actores usa PUB/SUB. 
Las GA de ambas sedes intercambian actualizaciones mediante replicaci√≥n peri√≥dica y pueden continuar funcionando en modo degradado si una sede falla. 
Este despliegue garantiza disponibilidad, balanceo de carga y redundancia geogr√°fica, cumpliendo los principios b√°sicos de los sistemas distribuidos. 
```mermaid
graph LR 

subgraph PC_A M√°quina A - Sede 1 

GC1 

A1D 

A1R 

end 

subgraph PC_B M√°quina B - Sede 2 

GC2 

A2D 

A2R 

end 

subgraph PC_C M√°quina C - Clientes 

PSx 

end 

 

PSx --- GC1 

PSx --- GC2 

GC1 --- A1D 

GC1 --- A1R 

GC2 --- A2D 

GC2 --- A2R 
```

### Requisitos:

Java 17 o superior
Librer√≠a JeroMQ
Dos o m√°s m√°quinas en red local (LAN)
Archivos CSV y de carga en la carpeta data/

### Estructura de carpetas:
```
Lab3/
‚îÇ‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ Gestor_Almacenamiento/
|        ‚îú‚îÄ‚îÄBaseDatos.java
|        ‚îú‚îÄ‚îÄEjemplar.java
|        ‚îú‚îÄ‚îÄGestorAlmacenamiento.java
|        ‚îú‚îÄ‚îÄGestorAlmacenamientompl.java
|        ‚îú‚îÄ‚îÄLibro.java
|        ‚îú‚îÄ‚îÄServidorGA.java
|   ‚îú‚îÄ‚îÄ Gestor_Carga/
|        ‚îú‚îÄ‚îÄActorClient.java
|        ‚îú‚îÄ‚îÄBibliotecaGC.java
|        ‚îú‚îÄ‚îÄBibliotecaGClmpl.java
|        ‚îú‚îÄ‚îÄMessage.java
|        ‚îú‚îÄ‚îÄServidorGC.java
‚îÇ   ‚îú‚îÄ‚îÄ ClienteBatch.java
‚îÇ   ‚îú‚îÄ‚îÄ libros.txt
‚îÇ   ‚îú‚îÄ‚îÄ peticiones.txt
‚îÇ‚îÄ‚îÄ README.md
```

## Diagrama de secuencia
El Diagrama de Secuencia representa el flujo completo de interacci√≥n entre los componentes del sistema distribuido durante la ejecuci√≥n de una operaci√≥n (ya sea renovaci√≥n o devoluci√≥n). 
```mermaid
sequenceDiagram
    participant PS as Proceso Solicitante
    participant GC as Gestor de Carga
    participant A as Actor (Renovaci√≥n / Devoluci√≥n)
    participant GA as Gestor de Almacenamiento

    PS->>GC: Enviar solicitud (RENOVACI√ìN / DEVOLUCI√ìN)
    GC-->>PS: Respuesta inmediata (202 Aceptada)
    GC->>A: Publicar mensaje (topic: tipo de operaci√≥n)
    A->>GA: Ejecutar actualizaci√≥n en BD local
    GA-->>A: Confirmar actualizaci√≥n OK/ERROR
    A-->>GC: Notificar resultado (opcional)
```


## ‚öôÔ∏è Ejecuci√≥n paso a paso
### 1. Compilar
Aseg√∫rese de tener instalado Java 17 o superior y la librer√≠a JeroMQ.
Desde la ra√≠z del proyecto, ejecute los siguientes comandos:
```
# Compilar todo el c√≥digo fuente
javac -cp .:jeromq-0.5.2.jar src/**/*.java -d bin
# Ejecutar los componentes seg√∫n el rol y la m√°quina
```

### 2. Ejecutar
#### M√°qu√≠na A (Sede 1)
```
# Iniciar Gestor de Almacenamiento 1
java -cp bin:jeromq-0.5.2.jar Gestor_Almacenamiento.ServidorGA 1

# Iniciar Gestor de Carga 1
java -cp bin:jeromq-0.5.2.jar Gestor_Carga.ServidorGC 1
```
#### M√°quina B (Sede 2)
```
java -cp bin:jeromq-0.5.2.jar Gestor_Almacenamiento.ServidorGA 2
java -cp bin:jeromq-0.5.2.jar Gestor_Carga.ServidorGC 2
```

#### M√°quina C (Clientes)
```
# Ejecutar m√∫ltiples procesos solicitantes desde archivos de carga
java -cp bin:jeromq-0.5.2.jar ClienteBatch data/peticiones_sede1.txt
java -cp bin:jeromq-0.5.2.jar ClienteBatch data/peticiones_sede2.txt
```
Cada cliente puede ejecutarse con diferente n√∫mero de hilos o solicitudes para generar carga variable.
Los logs y resultados se almacenan autom√°ticamente en `/data/logs/`.

## üìä Pruebas y m√©tricas

Las pruebas se ejecutaron siguiendo el protocolo definido en el informe t√©cnico, validando funcionalidad, concurrencia, tolerancia a fallos y rendimiento.
A continuaci√≥n se resumen las verificaciones m√°s relevantes:


|    M√©trica  |   Descripci√≥n  |   Resultado observado  |
|-------------|----------------|------------------------|
| **Latencia promedio** | Tiempo entre solicitud del PS y confirmaci√≥n del GC | 85 ‚Äì 120 ms |
| **Tiempo total de operaci√≥n (GC‚ÜíActor‚ÜíGA)** | Duraci√≥n completa del procesamiento de una transacci√≥n | 150 ‚Äì 180 ms |
| **Throughput** | Solicitudes procesadas por segundo | 45 ‚Äì 60 msg/s |
| **Tasa de √©xito** | Porcentaje de solicitudes completadas sin error | 99.5 % |
| **Desviaci√≥n est√°ndar de latencia** | Variabilidad en el tiempo de respuesta | ¬± 20 ms |
| **Retardo de replicaci√≥n** | Diferencia temporal entre GA primario y r√©plica | 2 ‚Äì 3 s |
| **Uso de CPU (GC multihilo)** | Carga promedio del proceso durante ejecuci√≥n simult√°nea | 65 ‚Äì 75 % |

**Conclusi√≥n:**  
El sistema mantiene **alta disponibilidad, baja latencia y consistencia eventual estable** incluso con 10 procesos solicitantes por sede.  
El uso de **ZeroMQ con asincron√≠a controlada** permiti√≥ mantener el throughput por encima del 90 % del caso base sin p√©rdida de mensajes.

### An√°lisis de resultados

Los resultados demuestran que la arquitectura distribuida propuesta logra un equilibrio entre **rendimiento, consistencia y tolerancia a fallos**.  
La **latencia baja** confirma la eficiencia del esquema as√≠ncrono basado en ZeroMQ, mientras que la **alta tasa de √©xito** evidencia la confiabilidad de la comunicaci√≥n entre procesos.  
El retardo de replicaci√≥n dentro de los rangos esperados garantiza **consistencia eventual estable**, y el consumo moderado de CPU en modo multihilo muestra que el sistema puede **escalar horizontalmente** sin degradar el desempe√±o.  
En conjunto, estas m√©tricas validan que el sistema cumple los **requisitos no funcionales** definidos en el dise√±o.
